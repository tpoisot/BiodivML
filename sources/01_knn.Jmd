---
title: k-Nearest Neighbors from scratch
layout: presentation
permalink: knn
---

# k-Nearest Neighbors from scratch

## That's not my penguin!

---

# Class agenda

## What we will do

- Write a $k$-NN function *from scratch*
- Use it to make predictions
- Start thinking about validation

--

## Why we will do it

- $k$-NN is a nice way to understand how *Julia* works
- It's a very intuitive algorithm
- It often works quite well!

---

# Setting up the environment

We will not need a lot more than for the previous module:

```julia
using DataFrames, DataFramesMeta
import CSV
import Cairo, Fontconfig
using Gadfly
```

To simplify our work, we will add the `StatsBase` package, which will make the
code nicer to write:

```julia
using Statistics
using StatsBase
```

---

# Loading the data

We will get the `penguins` data from the previous module -- as a reminder, we
can load them using *pipes*:

```julia; results="hidden"
penguins = 
    joinpath("data", "penguins.csv") |>
    CSV.File |>
    DataFrame |>
    dropmissing
```

Note that we add `dropmissing` (about 10 records) to avoid having to deal with
the issue of `Missing` data (for now).

---

class: split-40

# That's not my penguin!

.column[
![nmp](assets/notmypenguin.jpg)
]

.column[But how do we know if this is, in fact, our penguin?

How can we compare a penguin to other individuals?

Can we find out to which group any arbitrary penguin belongs?

**Yes**! Using $k$-NN.]

---

class: split-50

# OK so... what is k-NN anyways?

.column[
- A **non-parametric** method for **classification** and **regression**
- The **class membership** or **property value** for an unknown point (*object*) is based on the properties of its neighbors
- The **nearest neighbors** of a point (*instance*) are based on distance between values (*features*)
- The final decision follows a (more or less complex) **voting system**
]

--

.column[
- $k$-NN needs **no training**
- $k$ is an **hyper-parameter**, which makes the process more or less sensitive to data noise
- Because $k$-NN relies on *distances between points*, measurements in different units shouts be standardized
]

---

# Working our way through the terminology

The data we will use for this module is going to be the *four quantitative
measurements* from `penguins`. The first step will be to convert them into a
matrix of **features**:

```julia
features = permutedims(Matrix(penguins[!,[:culmen_depth, :culmen_length, :flipper_length, :bodymass]]))
```

Every *row* is a **feature**, and every *column* is an **instance**. This is a
convention: we deal with features as vectors, and **V**ectors are **V**ertical.

The vector of features for the first instance is:

$$
v_1 = [`j features[1,1]`, `j features[2,1]`, `j features[3,1]`, `j features[4,1]`]^T
$$

---

class: split-50

# Transforming the data

The data are not expressed in the same unit - we will apply a simple $z$-score
transformation.


.column[
```julia
μ = vec(mean(features, dims=2))
```
]

.column[
```julia
σ = vec(std(features, dims=2))
```
]

--

We can now work on the version of the data where every features as mean 0 and
unit standard deviation:

```julia
nf = (features .- μ)./σ
```

---

class: split-30

# But how does k-NN *works*?

.column[
- Get measurements for an object with **unknwon membership**
- Find out which $k$ known instances have the **closest features**
- Take a **majority consensus** of the class of the neighbors
]

--

.column[
If we have measured the following penguin:

```julia; results="hidden"
pingoo = [12.4, 46.7, 215.3, 4842.0]
```

what is its species, knowing all the data we already have?

This will require a **data transformation** to express the measurements (in
biological units) in the unitless **features space**:

```julia; results="hidden"
nd = (pingoo .- μ)./σ
```

$$
v_\text{pingoo} = [`j round(nd[1]; digits=2)`, `j round(nd[2]; digits=2)`, `j round(nd[3]; digits=2)`, `j round(nd[4]; digits=2)`]^T
$$

]

---

class: split-50

# Measuring the distances

.column[
We can very easily use the Euclidean distance:

```julia; results="hidden"
distances = vec(sqrt.(sum((nf .- nd).^2.0; dims=1)))
```

Let's also plot it to look at the distribution:

```julia
plot(
    y = sort(distances),
    Geom.line,
    Guide.xlabel("Rank of neighbor"),
    Guide.ylabel("Distance")
) |> PNG("figures/knndist.png", dpi=600)
```

Note that $k$-NN does not require to set a *distance* cutoff (so we don't care
too much about the distance distribution)!
]

.column[
![](figures/knndist.png)
]

---

# Getting the class membership of neighbors

We can use `sortperm` to return a *sorted ordering* of the distance vector, and
then use `findall` to get the position of the distances that are the $k$
smallest:

```julia
k = 5
neighbors = findall(sortperm(distances) .<= k)
neighbors'
```

Because we know where the **labels** are stored (`penguins.species`), we can get
the pool of possible species for our object:

```julia
penguins.species[neighbors]
```

---

# Assigning our penguin to a class

Voting is, at this point, as simple as counting the number of times any species
was recommended:

```julia
votes = countmap(penguins.species[neighbors])
```

We can use some basic `sort`ing of the votes to get the most likely species for
the sample:

```julia
first(sort(collect(votes), by = (x) -> x.second, rev=true)).first
```

---

# The "landscape" of k-NN predictions

We will assume that we know of *two* values, and ignore two others:


```julia
ftval = LinRange(-3, 3, 90)
ftcomb = vec(collect(Base.product(ftval, ftval)))
decisions = []
for (f1, f2) in ftcomb
    tv = [f1, f2, 0.0, 0.0]
    distances = vec(sqrt.(sum((nf .- tv).^2.0; dims=1)))
    neighbors = findall(sortperm(distances) .<= 5)
    votes = countmap(penguins.species[neighbors])
    decision = first(sort(collect(votes), by = (x) -> x.second, rev=true)).first
    push!(decisions, decision)
end
```

---

class: split-50

# Visualizing the predictions (k=5)

.column[
```julia
f1 = [first(c) for c in ftcomb]
f2 = [last(c) for c in ftcomb]
plot(
    x = f1, y = f2,
    color=decisions,
    Geom.rectbin,
    Guide.xlabel("Culmen depth (relative)"),
    Guide.ylabel("Culmen length (relative)"),
    Coord.cartesian(
        xmin=-3, xmax=3, ymin=-3, ymax=3, fixed=true
    )
) |> PNG("figures/knnsim.png", dpi=600) 
```

]

.column[
![](figures/knnsim.png)
]

---

# Intermezzo: turning k-NN into a function

```julia; results="hidden"
function knn(v::Vector{TF}, features::Matrix{TF}, labels::Vector{TL}; k::Integer=5) where {TF <: Number, TL}
    @assert length(v) == size(features, 1)
    @assert length(labels) == size(features, 2)
    @assert 1 <= k <= length(labels)

    Δ = vec(sqrt.(sum((v .- features).^2.0; dims=1)))

    neighbors = findall(sortperm(Δ) .<= k)
    votes = countmap(labels[neighbors])
    decision = first(sort(collect(votes), by = (x) -> x.second, rev=true)).first

    return decision
end
```

```julia; results="hidden"
labels = vec(String.(penguins.species))
features = nf
```

---

# Cross validation

We want to **evaluate** the model on data it **has not seen before**. We call
this **cross-validation**.

--

**Holdout**: the dataset is split in two (training and testing), and we measure
the performance by applying to the model on the testing set, but only informing
it of the training set.

--

**K-fold**: we divide the dataset in $K$ samples, train it on
$K-1$, and evaluate on the remaining one.

--

**Leave-One-Out**: we apply K-fold validation, where $K = n-1$, so that *every
single object* in the dataset is evaluated as a testing set.

---

# Measuring accuracy

One important decision to make when evaluating a model is to decide *which
function is used to measure its performance*.

We will see a lot more when working on binary classifiers.

For the moment, let's have a look at the accurracy function: the proportion of
correct guesses.

--

```julia; results="hidden"
function accuracy(x::Vector{T}, y::Vector{T}) where {T}
    @assert length(x) == length(y)
    return sum(x .== y)/length(x)
end
```

---

# Leave-One-Out cross validation

```julia
guesses = similar(labels)

function loocv!(guesses, features, labels; k=3)
    for i in 1:size(features, 2)
        tf = features[:, setdiff(1:end, i)]
        tl = labels[setdiff(1:end, i)]
        guesses[i] = knn(vec(features[:,i]), tf, tl; k=k)
    end
    return accuracy(guesses, labels)
end

function loocv(features, labels; k=3)
    guesses = similar(labels)
    return loocv!(guesses, features, labels; k=k)
end

loocv!(guesses, features, labels)
```

---

class: split-50

# Can we find the "best" k?

.column[
```julia
K = collect(1:20)

acc = [
    loocv!(
        guesses, features, labels; k=k
    ) for k in K
]

plot(
    x = K, y = acc,
    Geom.point,
    Geom.line,
    Guide.xlabel("Number of neighbors"),
    Guide.ylabel("Accuracy"),
) |> PNG("figures/knnloo.png", dpi=600) 
```

]

.column[
![](figures/knnloo.png)
]
