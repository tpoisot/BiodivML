---
title: k-Nearest Neighbors from scratch
layout: presentation
permalink: knn
---

# k-Nearest Neighbors from scratch

## That's not my penguin!

---

# Class agenda

## What we will do

- Write a $k$-NN function *from scratch*
- Use it to make predictions
- Start thinking about validation

--

## Why we will do it

- $k$-NN is a nice way to understand how *Julia* works
- It's a very intuitive algorithm
- It often works quite well!

---

# Setting up the environment

We will not need a lot more than for the previous module:

```julia
using DataFrames, DataFramesMeta
import CSV
import Cairo, Fontconfig
using Gadfly
```

To simplify our work, we will add the `StatsBase` package, which will make the
code nicer to write:

```julia
using Statistics
using StatsBase
```

---

# Loading the data

We will get the `penguins` data from the previous module -- as a reminder, we
can load them using *pipes*:

```julia; results="hidden"
penguins = 
    joinpath("data", "penguins.csv") |>
    CSV.File |>
    DataFrame |>
    dropmissing
```

Note that we add `dropmissing` (about 10 records) to avoid having to deal with
the issue of `Missing` data (for now).

---

class: split-40

# That's not my penguin!

.column[
![nmp](assets/notmypenguin.jpg)
]

.column[But how do we know if this is, in fact, our penguin?

How can we compare a penguin to other individuals?

Can we find out to which group any arbitrary penguin belongs?

**Yes**! Using $k$-NN.]

---

class: split-50

# OK so... what is k-NN anyways?

.column[
- A **non-parametric** method for **classification** and **regression**
- The **class membership** or **property value** for an unknown point (*object*) is based on the properties of its neighbors
- The **nearest neighbors** of a point (*instance*) are based on distance between values (*features*)
- The final decision follows a (more or less complex) **voting system**
]

--

.column[
- $k$-NN needs **no training**
- $k$ is an **hyper-parameter**, which makes the process more or less sensitive to data noise
- Because $k$-NN relies on *distances between points*, measurements in different units shouts be standardized
]

---

# Working our way through the terminology

The data we will use for this module is going to be the *four quantitative
measurements* from `penguins`. The first step will be to convert them into a
matrix of **features**:

```julia
features = permutedims(Matrix(penguins[!,[:culmen_depth, :culmen_length, :flipper_length, :bodymass]]))
```

Every *row* is a **feature**, and every *column* is an **instance**. This is a
convention: we deal with features as vectors, and **V**ectors are **V**ertical.

The vector of features for the first instance is:

$$
v_1 = [`j features[1,1]`, `j features[2,1]`, `j features[3,1]`, `j features[4,1]`]^T
$$

---

class: split-50

# Transforming the data

The data are not expressed in the same unit - we will apply a simple $z$-score
transformation.


.column[
```julia
μ = vec(mean(features, dims=2))
```
]

.column[
```julia
σ = vec(std(features, dims=2))
```
]

--

We can now work on the version of the data where every features as mean 0 and
unit standard deviation:

```julia
nf = (features .- μ)./σ
```

---

class: split-30

# But how does k-NN *works*?

.column[
- Get measurements for an object with **unknwon membership**
- Find out which $k$ known instances have the **closest features**
- Take a **majority consensus** of the class of the neighbors
]

--

.column[
If we have measured the following penguin:

```julia; results="hidden"
pingoo = [12.4, 46.7, 215.3, 4842.0]
```

what is its species, knowing all the data we already have?

This will require a **data transformation** to express the measurements (in
biological units) in the unitless **features space**:

```julia; results="hidden"
nd = (pingoo .- μ)./σ
```

$$
v_\text{pingoo} = [`j round(nd[1]; digits=2)`, `j round(nd[2]; digits=2)`, `j round(nd[3]; digits=2)`, `j round(nd[4]; digits=2)`]^T
$$

]

---

class: split-50

# Measuring the distances

.column[
We can very easily use the Euclidean distance:

```julia; results="hidden"
distances = vec(sqrt.(sum((nf .- nd).^2.0; dims=1)))
```

Let's also plot it to look at the distribution:

```julia
plot(
    y = sort(distances),
    Geom.line,
    Guide.xlabel("Rank of neighbor"),
    Guide.ylabel("Distance")
) |> PNG("figures/knndist.png", dpi=600)
```

Note that $k$-NN does not require to set a *distance* cutoff (so we don't care
too much about the distance distribution)!
]

.column[
![](figures/knndist.png)
]

---

# Getting the class membership of neighbors

We can use `sortperm` to return a *sorted ordering* of the distance vector, and
then use `findall` to get the position of the distances that are the $k$
smallest:

```julia
k = 5
neighbors = findall(sortperm(distances) .<= k)
neighbors'
```

Because we know where the **labels** are stored (`penguins.species`), we can get
the pool of possible species for our object:

```julia
penguins.species[neighbors]
```

---

# Assigning our penguin to a class

Voting is, at this point, as simple as counting the number of times any species
was recommended:

```julia
votes = countmap(penguins.species[neighbors])
```

We can use some basic `sort`ing of the votes to get the most likely species for
the sample:

```julia
first(sort(collect(votes), by = (x) -> x.second, rev=true)).first
```

---

# The "landscape" of k-NN predictions

We will assume that we know of *two* values, and ignore two others:


```julia
ftval = LinRange(-2, 2, 50)
ftcomb = vec(collect(Base.product(ftval, ftval)))
decisions = []
for (f1, f2) in ftcomb
    tv = [f1, f2, 0.0, 0.0]
    distances = vec(sqrt.(sum((nf .- tv).^2.0; dims=1)))
    neighbors = findall(sortperm(distances) .<= 5)
    votes = countmap(penguins.species[neighbors])
    decision = first(sort(collect(votes), by = (x) -> x.second, rev=true)).first
    push!(decisions, decision)
end
```

---

class: split-50

# Visualizing the predictions (k=5)

.column[
```julia
f1 = [first(c) for c in ftcomb]
f2 = [last(c) for c in ftcomb]
plot(
    x = f1, y = f2,
    color=decisions,
    Geom.rectbin,
    Guide.xlabel("Culmen depth (relative)"),
    Guide.ylabel("Culmen length (relative)"),
    Coord.cartesian(
        xmin=-2, xmax=2, ymin=-2, ymax=2, fixed=true
    )
) |> PNG("figures/knnsim.png", dpi=600) 
```

]

.column[
![](figures/knnsim.png)
]
