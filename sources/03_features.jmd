---
title: Optimizing features
layout: presentation
permalink: features
---

# Optimizing features

## PCA and all that

---

# Class agenda

## What we will do

- Apply some transformation on the penguins dataset
- Discuss the difference between PCA in ecology and machine learning
- See the effect of feature transformation on model performance

--

## Why we will do it

- Very few features are directly usable
- Models usually work best when we correct for statistical artifacts
- It's (sometimes) a good way to have fewer dimensions

---

# Setting up the environment

We will not need a lot more than for the previous module:

```julia
using DataFrames, DataFramesMeta
import CSV
import Cairo, Fontconfig
using Gadfly
```

We will add the `MultivariateStats` and `DecisionTree` packages to help.

```julia
using Statistics
using StatsBase
using MultivariateStats
using DecisionTree
```

---

# Loading the data

We will get the `penguins` data from the previous module -- as a reminder, we
can load them using *pipes*:

```julia; results="hidden"
penguins = 
    joinpath("data", "penguins.csv") |>
    CSV.File |>
    DataFrame |>
    dropmissing
```

Note that we add `dropmissing` (about 10 records) to avoid having to deal with
the issue of `Missing` data (for now).

~~~julia; results="hidden"
features = permutedims(Matrix(penguins[!,[:culmen_depth, :culmen_length, :flipper_length, :bodymass]]))
labels = penguins.species
~~~

---

# Training and testing

We will split our dataset into a training and testing set.

~~~julia; results="hidden"
test_index = sample(1:length(labels), 100, replace=false)
train_index = filter(i -> !(i in test_index), 1:length(labels))
~~~

To avoid having too many variables, we will carry the test/train
features/labels together in tuples:

~~~julia
# DecisionTree follows the 'wrong' convention for features...
testset = (features[:,test_index]', vec(labels[test_index]))
trainset = (features[:,train_index]', vec(labels[train_index]))
~~~

---

class: split-50

# A baseline model

We will use a regression tree (using CART, from `DecisionTree`) to classify
penguins based on features:

.column[
~~~julia
model = build_tree(reverse(trainset)...)
model = prune_tree(model, 0.9)
print_tree(model, 3)
~~~
]

.column[
~~~julia
prediction = apply_tree(model, first(testset))
cm_bas = confusion_matrix(last(testset), prediction)
~~~
]

---

class: split-50

# Centering and Standardizing

The data are not expressed in the same unit - we will apply a simple $z$-score
transformation, as we did with our bespoke $k$-NN:

.column[
```julia
# We need to express these as row vectors
mn = vec(mean(features, dims=2))'
st = vec(std(features, dims=2))'
```
]

.column[
~~~julia
model = build_tree(last(trainset), (first(trainset).-mn)./st)
model = prune_tree(model, 0.9)
prediction = apply_tree(model, (first(testset).-mn)./st)
cm_cen = confusion_matrix(last(testset), prediction)
~~~
]

---

# Centering and Standardizing - discussion

- There is not a great difference in performance
- Why?

---

# Covariance (it's a problem)

Data have a *covariance*:

~~~julia
cov(features')
~~~

In short, knowing something about a variable *might* tell us something about another variable.

---

# Whitening - fixing covariance

Whitening transforms creates a set of *new variables*, whose covariance matrix is the *identity matrix*.

These new variables are uncorrelated, and all have unit variance.

~~~julia
W = fit(Whitening, first(trainset)')
W.W
~~~

---

# Whitening

~~~julia
model = build_tree(last(trainset), MultivariateStats.transform(W, first(trainset)')')
model = prune_tree(model, 0.9)
prediction = apply_tree(model, MultivariateStats.transform(W, first(testset)')')
cm_whi = confusion_matrix(last(testset), prediction)
~~~

---

# Whitening - transformations are model

Note that we can transform *any* vector with 4 elements corresponding to the features into a set of random variables.

The transformation *is* a model!

This will be important for PCA.

---

# Whitening - discussion

- There is a small dip in performance
- Why?

---

# PCA - reprojecting the variables

~~~julia
ctrain = ((first(trainset).-mn)./st)'
ctest = ((first(testset).-mn)./st)'
P = fit(PCA, ctrain)
~~~

~~~julia
projection(P)
~~~

| Feature            | Value                 |
|--------------------|-----------------------|
| Input dim.         | `j indim(P)`          |
| Output dim.        | `j outdim(P)`         |
| Variance preserved | `j principalratio(P)` |

---

class: split-50

# PCA - dimensionality reduction

Whitening kept the *same* number of variables.

--

PCA can *reduce* the number of variables, by keeping *just enough* to explain
a set proportion of variance. Here, starting from `j indim(P)` features,
we can explain 99% of variance with `j outdim(P)` axis.

--

What do you think would happen with the "raw" features?

---

# PCA

~~~julia
model = build_tree(last(trainset), MultivariateStats.transform(P, ctrain)')
model = prune_tree(model, 0.9)
prediction = apply_tree(model, MultivariateStats.transform(P, ctest)')
cm_pca = confusion_matrix(last(testset), prediction)
~~~

---

# PCA - discussion

- The performance increased *a little*
- Why?

---

# Summary

| Model                | Accuracy            | Cohen's Kappa    |
|----------------------|---------------------|------------------|
| baseline             | `j cm_bas.accuracy` | `j cm_bas.kappa` |
| center + standardize | `j cm_cen.accuracy` | `j cm_cen.kappa` |
| Whitening            | `j cm_whi.accuracy` | `j cm_whi.kappa` |
| PCA                  | `j cm_pca.accuracy` | `j cm_pca.kappa` |

--

## What's going on?

--

- decision trees are prone to **overfitting** (we'll get back to this with neural networks)
- the classes are essentially **linearly separable** - we can draw lines to split the dataset
- sometimes you don't **need** the fancy features transformation!
