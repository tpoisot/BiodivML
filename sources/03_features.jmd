---
title: Optimizing features
layout: presentation
permalink: features
---

# Optimizing features

## PCA and all that

---

# Class agenda

## What we will do

- Apply some transformation on the penguins dataset
- Discuss the difference between PCA in ecology and machine learning
- See the effect of feature transformation on model performance

--

## Why we will do it

- Very few features are directly usable
- Models usually work best when we correct for statistical artifacts
- It's (sometimes) a good way to have fewer dimensions

---

# Setting up the environment

We will not need a lot more than for the previous module:

```julia
using DataFrames, DataFramesMeta
import CSV
import Cairo, Fontconfig
using Gadfly
```

We will add the `MultivariateStats` and `DecisionTree` packages to help.

```julia
using Statistics
using StatsBase
using MultivariateStats
using DecisionTree
```

---

# Loading the data

We will get the `penguins` data from the previous module -- as a reminder, we
can load them using *pipes*:

```julia; results="hidden"
penguins = 
    joinpath("data", "penguins.csv") |>
    CSV.File |>
    DataFrame |>
    dropmissing
```

Note that we add `dropmissing` (about 10 records) to avoid having to deal with
the issue of `Missing` data (for now).

~~~julia; results="hidden"
features = permutedims(Matrix(penguins[!,[:culmen_depth, :culmen_length, :flipper_length, :bodymass]]))
labels = penguins.species
~~~

---

# Training and testing

We will split our dataset into a training and testing set.

~~~julia; results="hidden"
test_index = sample(1:length(labels), 50, replace=false)
train_index = filter(i -> !(i in test_index), 1:length(labels))
~~~

To avoid having too many variables, we will carry the test/train
features/labels together in tuples:

~~~julia
# DecisionTree follows the 'wrong' convention for features...
testset = (features[:,test_index]', vec(labels[test_index]))
trainset = (features[:,train_index]', vec(labels[train_index]))
~~~

---

class: split-50

# A baseline model

We will use a regression tree (using CART, from `DecisionTree`) to classify
penguins based on features:

.column[
~~~julia
model = build_tree(reverse(trainset)...)
model = prune_tree(model, 0.9)
print_tree(model, 3)
~~~
]

.column[
~~~julia
prediction = apply_tree(model, first(testset))
confusion_matrix(last(testset), prediction)
~~~
]

---

class: split-50

# Centering and Standardizing

The data are not expressed in the same unit - we will apply a simple $z$-score
transformation, as we did with our bespoke $k$-NN:

.column[
```julia
# We need to express these as row vectors
mn = vec(mean(features, dims=2))'
st = vec(std(features, dims=2))'
```
]

.column[
~~~julia
model = build_tree(last(trainset), (first(trainset).-mn)./st)
model = prune_tree(model, 0.9)
prediction = apply_tree(model, (first(testset).-mn)./st)
confusion_matrix(last(testset), prediction)
~~~
]

---

# Centering and Standardizing - discussion

- There is not a great difference in performance
- Why?

---
